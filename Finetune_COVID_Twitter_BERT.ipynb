{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Finetune COVID-Twitter-BERT",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/digitalepidemiologylab/covid-twitter-bert/blob/master/Finetune_COVID_Twitter_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wtjs1QDb3DX",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"right\" width=\"450px\" src=\"https://github.com/digitalepidemiologylab/covid-twitter-bert/raw/master/images/COVID-Twitter-BERT-medium.png\">\n",
        "\n",
        "# Finetuning COVID-Twitter-BERT\n",
        "The current notebook is inspired by the [BERT End to End (Fine-tuning + Predicting) with Cloud TPU](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). \n",
        "\n",
        "The notebook above also has a detailed description on how to set up a GCP (Google Compute Engine) account and a GCS (Google Cloud Storage) bucket. \n",
        "\n",
        "##Before proceeding\n",
        "* Create a copy of this notebook by going to \"File - Save a Copy in Drive\"\n",
        "* Create the bucket as described above. You will need full write access to this GSC bucket and you will need to enter the address in field below.\n",
        "\n",
        "\n",
        "\n",
        "## Set Bucket Name\n",
        "Please provide the name of your private GSC bucket below. You need full access to this bucket both for storing the data and for storing the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYkaAlJNfhul",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = 'gs://' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26aAaRXtNs7U",
        "colab_type": "text"
      },
      "source": [
        "## Set up your TPU environment\n",
        "This section performs a few tasks like importing some libraries, authenticating and setting up logging.\n",
        "\n",
        "It then activates the TPU runtime. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "191zq3ZErihP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys, datetime, time, json, math, logging,datetime\n",
        "import tensorflow as tf\n",
        "from google.colab import auth\n",
        "import tensorflow_gcs_config\n",
        "\n",
        "#Authenticate to be able to store objects in bucket\n",
        "auth.authenticate_user()\n",
        "\n",
        "assert BUCKET_NAME\n",
        "\n",
        "# set up logging\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)-5.5s] [%(name)-12.12s]: %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# remove duplicate logger\n",
        "tf_logger = tf.get_logger()\n",
        "if len(tf_logger.handlers) > 1 :\n",
        "  tf_logger.handlers.pop()\n",
        "\n",
        "# set up TPU strategy\n",
        "logger.info(f'Running tensorflow version {tf.__version__}')\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "logger.info(f'TPU address is {TPU_ADDRESS}')\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "# authenticate cloud bucket\n",
        "tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "# set TF hub caching to bucket\n",
        "os.environ['TFHUB_CACHE_DIR'] = os.path.join(f'{BUCKET_NAME}', 'tmp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUBP35oCDmbF",
        "colab_type": "text"
      },
      "source": [
        "## Clone Repository\n",
        "​\n",
        "The following step clones the CT-BERT repository. The repository also contains the Tensorflow 2.2  compatible version of the  official tensorflow/models. It then installs Sentencepiece and imports the necessary dependencies from the repocitory. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzwke0sxS6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!test -d covid-twitter-bert || git clone https://github.com/digitalepidemiologylab/covid-twitter-bert.git --recursive\n",
        "%cd covid-twitter-bert\n",
        "\n",
        "# Needed for tokenization\n",
        "!pip install sentencepiece\n",
        "\n",
        "sys.path.append('tensorflow_models')\n",
        "from official.nlp.bert import bert_models\n",
        "from official.utils.misc import distribution_utils\n",
        "from official.nlp.bert import configs as bert_configs\n",
        "from official.modeling import performance\n",
        "from official.nlp.bert import input_pipeline\n",
        "from official.utils.misc import keras_utils\n",
        "from official.nlp import optimization\n",
        "from config import PRETRAINED_MODELS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5igYNFrndau",
        "colab_type": "text"
      },
      "source": [
        "# Download GLUE Dataset\n",
        "This downloads the SST-2 task from the GLUE dataset. This is simple tsv-files (tab separated csv). You can examine them and download them from the Colab disk.\n",
        "\n",
        "You might want to test CT-BERT on your own data. All you need to do is to rewrite this cell. Prepare your annotated data in the format: \n",
        "\n",
        "`text \\t label \\n`\n",
        "\n",
        "After this is done, split it in a training file (train.tsv) and a development file (dev.tsv) and upload it to /content/DATA_DIR/TASK_NAME/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGGAKhiIWe_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TASK_NAME = \"SST-2\" #This is used as directory names both locally and in the bucket\n",
        "DATA_DIR = '/content/glue_data' #The actual data is stored in the subfolder called TASK_NAME\n",
        "\n",
        "!test -f download_glue_data.py || wget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
        "!python download_glue_data.py --tasks SST --data_dir {DATA_DIR}\n",
        "\n",
        "print(f\"The following datafiles are downloaded to {DATA_DIR}/{TASK_NAME}:\")\n",
        "!ls /content/glue_data/SST-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnhZE25R-Tee",
        "colab_type": "text"
      },
      "source": [
        "# Choose Model \n",
        "Most likely you will like to leave this as covid-twitter-bert. However you can change this to compare with other pretrained models. We need to know which model to prepare the dataset for. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_FYB3239o6l",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "MODEL_CLASS = 'covid-twitter-bert' #@param [\"covid-twitter-bert\", \"bert_large_uncased_wwwm\", \"bert_large_uncased\"]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9b5JYLx10QX",
        "colab_type": "text"
      },
      "source": [
        "# Create TFRecord Dataset\n",
        "This converts your tab-separeted files to a binary tfrecord-file. These files are uploaded to the bucket you created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oW8wlm4nDkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert MODEL_CLASS\n",
        "\n",
        "#Get path for vocab-file and config file.\n",
        "VOCAB_FILE = os.path.join('.', 'vocabs', PRETRAINED_MODELS[MODEL_CLASS]['vocab_file'])\n",
        "CONFIG_FILE = os.path.join('.', 'configs', PRETRAINED_MODELS[MODEL_CLASS]['config'])\n",
        "\n",
        "!PYTHONPATH=tensorflow_models python tensorflow_models/official/nlp/data/create_finetuning_data.py \\\n",
        " --input_data_dir={DATA_DIR}/{TASK_NAME}/ \\\n",
        " --vocab_file={VOCAB_FILE} \\\n",
        " --train_data_output_path={BUCKET_NAME}/data/{TASK_NAME}/train.tf_record \\\n",
        " --eval_data_output_path={BUCKET_NAME}/data/{TASK_NAME}/eval.tf_record \\\n",
        " --meta_data_file_path={BUCKET_NAME}/data/{TASK_NAME}/meta.json \\\n",
        " --fine_tuning_task_type=classification \\\n",
        " --max_seq_length=96 \\\n",
        " --classification_task_name=SST-2 #Even when using your own data, leave this as SST-2. It will tell the script to create a standard classification task dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrB2ooXRH4DO",
        "colab_type": "text"
      },
      "source": [
        "##Set HyperParameters for Training\n",
        "The default settings are good in most cases. Adjust the number of epochs between 3 and 10. Smaller and more unbalanced datasets will require more epochs to finetune. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWYkRkfvDWdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You might want to change some of these parameters\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_PROPORTION = 0.1\n",
        "NUM_EPOCHS = 1\n",
        "TIME_HISTORY_LOG_STEPS = 10\n",
        "MAX_SEQ_LENGTH = 96   # CT-BERT is optimised for a sequence length of 96. This is sufficient for Twitter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6se7n8oy_Zom",
        "colab_type": "text"
      },
      "source": [
        "# Helper functions\n",
        "Defines a few extra helper functions that are needed for training. Since most users will not need to modify this, the code is hidden by default. You will still have to run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHgGBzvX_cOa",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def get_model_config():\n",
        "    \"\"\"Reads BERT config\"\"\"\n",
        "    config = bert_configs.BertConfig.from_json_file(CONFIG_FILE)\n",
        "    return config\n",
        "  \n",
        "def get_input_meta_data():                                                                                                                                                   \n",
        "    \"\"\"Reads meta data file (containing information about finetune data)\"\"\"\n",
        "    with tf.io.gfile.GFile(os.path.join(DATA_DIR, 'meta.json'), 'rb') as reader:                                                                                                     \n",
        "        input_meta_data = json.loads(reader.read().decode('utf-8'))                                                                                                                  \n",
        "    return input_meta_data     \n",
        "\n",
        "def get_model(model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length, is_hub_module=False):                                                                   \n",
        "    \"\"\"Get classifier and core model (used to initialize from checkpoint)\"\"\"\n",
        "    if PRETRAINED_MODELS[MODEL_CLASS]['is_tfhub_model']:                                                                                                                        \n",
        "        hub_module_url = f\"https://tfhub.dev/{PRETRAINED_MODELS[MODEL_CLASS]['hub_url']}\"                                                                                       \n",
        "        hub_module_trainable = True                                                                                                                                                  \n",
        "    else:                                                                                                                                                                            \n",
        "        hub_module_url = None                                                                                                                                                        \n",
        "        hub_module_trainable = False                                                                                                                                                 \n",
        "    # Build BERT classifier model\n",
        "    classifier_model, core_model = bert_models.classifier_model(                                                                                                                     \n",
        "            model_config,                                                                                                                                                            \n",
        "            num_labels,                                                                                                                                                              \n",
        "            MAX_SEQ_LENGTH,                                                                                                                                                          \n",
        "            hub_module_url=hub_module_url,                                                                                                                                           \n",
        "            hub_module_trainable=hub_module_trainable)                                                                                                                               \n",
        "    # Create optimizer with linear decay learning rate + warmup\n",
        "    optimizer = optimization.create_optimizer(                                                                                                                                    \n",
        "            LEARNING_RATE,                                                                                                                                                      \n",
        "            steps_per_epoch * NUM_EPOCHS,                                                                                                                                       \n",
        "            warmup_steps)                                                                                                                                                           \n",
        "    classifier_model.optimizer = optimizer\n",
        "    return classifier_model, core_model     \n",
        "\n",
        "def get_loss_fn(num_classes):\n",
        "    \"\"\"Gets the classification loss function.\"\"\"\n",
        "    def classification_loss_fn(labels, logits):\n",
        "        \"\"\"Classification loss.\"\"\"\n",
        "        labels = tf.squeeze(labels)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        one_hot_labels = tf.one_hot(tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n",
        "        per_example_loss = -tf.reduce_sum(tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n",
        "        return tf.reduce_mean(per_example_loss)\n",
        "    return classification_loss_fn\n",
        "\n",
        "def get_dataset_fn(input_file_pattern, max_seq_length, global_batch_size, is_training=True):\n",
        "  \"\"\"Gets a closure to create a dataset.\"\"\"\n",
        "  def _dataset_fn(ctx=None):\n",
        "    \"\"\"Returns tf.data.Dataset for distributed BERT pretraining.\"\"\"\n",
        "    batch_size = ctx.get_per_replica_batch_size(\n",
        "        global_batch_size) if ctx else global_batch_size\n",
        "    dataset = input_pipeline.create_classifier_dataset(\n",
        "        input_file_pattern,\n",
        "        max_seq_length,\n",
        "        batch_size,\n",
        "        is_training=is_training,\n",
        "        input_pipeline_context=ctx)\n",
        "    return dataset\n",
        "  return _dataset_fn\n",
        "\n",
        "def get_metrics():\n",
        "    return [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK54QskWB6zU",
        "colab_type": "text"
      },
      "source": [
        "##Prepare the Model for Training\n",
        "A few more steps is needed to prepare the model for training. This is mainly getting and calculating the configs.\n",
        "\n",
        "At the end the model is loaded and compiled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYVYULZiKvUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from config import PRETRAINED_MODELS\n",
        "\n",
        "# Get configs\n",
        "model_config = get_model_config()\n",
        "\n",
        "# Setting some variables automatically\n",
        "RUN_NAME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S_%f')\n",
        "OUTPUT_DIR = f'{BUCKET_NAME}/runs/{RUN_NAME}'\n",
        "DATA_DIR = f'{BUCKET_NAME}/data/{TASK_NAME}'\n",
        "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "\n",
        "# Setting callbacks\n",
        "summary_dir = os.path.join(OUTPUT_DIR, 'summaries')\n",
        "summary_callback = tf.keras.callbacks.TensorBoard(summary_dir, profile_batch=0)\n",
        "checkpoint_path = os.path.join(OUTPUT_DIR, 'checkpoint')\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True)\n",
        "time_history_callback = keras_utils.TimeHistory(\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    log_steps=TIME_HISTORY_LOG_STEPS,\n",
        "    logdir=summary_dir)\n",
        "custom_callbacks = [summary_callback, checkpoint_callback, time_history_callback]\n",
        "\n",
        "# Meta data/label mapping\n",
        "input_meta_data = get_input_meta_data()\n",
        "label_mapping = None\n",
        "logger.info(f'Loaded training data meta.json file: {input_meta_data}')\n",
        "\n",
        "# Calculate steps, warmup steps and eval steps\n",
        "train_data_size = input_meta_data['train_data_size']\n",
        "num_labels = input_meta_data['num_labels']\n",
        "max_seq_length = input_meta_data['max_seq_length']\n",
        "steps_per_epoch = int(train_data_size / TRAIN_BATCH_SIZE)\n",
        "warmup_steps = int(NUM_EPOCHS * train_data_size * WARMUP_PROPORTION / TRAIN_BATCH_SIZE)\n",
        "eval_steps = int(math.ceil(input_meta_data['eval_data_size'] / EVAL_BATCH_SIZE))\n",
        "\n",
        "logger.info(f'Running {NUM_EPOCHS} epochs with {steps_per_epoch:,} steps per epoch')\n",
        "logger.info(f'Using warmup proportion of {WARMUP_PROPORTION}, resulting in {warmup_steps:,} warmup steps')\n",
        "logger.info(f'Using learning rate: {LEARNING_RATE}, training batch size: {TRAIN_BATCH_SIZE}, num_epochs: {NUM_EPOCHS}')\n",
        "\n",
        "# Generate dataset functions\n",
        "with tpu_strategy.scope():\n",
        "  train_input_fn = get_dataset_fn(\n",
        "      os.path.join(DATA_DIR, 'train.tf_record'),\n",
        "      MAX_SEQ_LENGTH,\n",
        "      TRAIN_BATCH_SIZE,\n",
        "      is_training=True)\n",
        "  eval_input_fn = get_dataset_fn(\n",
        "      os.path.join(DATA_DIR, 'eval.tf_record'),\n",
        "      MAX_SEQ_LENGTH,\n",
        "      EVAL_BATCH_SIZE,\n",
        "      is_training=False)\n",
        " \n",
        "# Get model\n",
        "with tpu_strategy.scope():\n",
        "  classifier_model, core_model = get_model(model_config, steps_per_epoch, warmup_steps, num_labels, max_seq_length)\n",
        "  optimizer = classifier_model.optimizer\n",
        "  loss_fn = get_loss_fn(num_labels)\n",
        "logger.info('The model is loaded!')\n",
        "\n",
        "#Compile the model\n",
        "logger.info(f'Compiling keras model...')\n",
        "with tpu_strategy.scope():\n",
        "  classifier_model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss_fn,\n",
        "      metrics=get_metrics())\n",
        "logger.info(f'The model is compiled')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMJ7AIWWEQTt",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model\n",
        "Finally we are ready to train the model. How long this takes depends on the size of your dataset and the number of epochs. Typical training times are 10-30 minutes on a TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKrAlCGdEbIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time_start = time.time()\n",
        "logger.info('Run training...')\n",
        "with tpu_strategy.scope():\n",
        "  history = classifier_model.fit(\n",
        "      x=train_input_fn(),\n",
        "      validation_data=eval_input_fn(),\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=NUM_EPOCHS,\n",
        "      validation_steps=eval_steps,\n",
        "      callbacks=custom_callbacks,\n",
        "      verbose=1)\n",
        "time_end = time.time()\n",
        "training_time_min = (time_end-time_start)/60\n",
        "logger.info(f'Finished training after {training_time_min:.1f} min')\n",
        "\n",
        "# show final results\n",
        "logger.info(f'Final results {history.history}')\n",
        "\n",
        "\n",
        "# The checkpoints should be stored in your Google bucket\n",
        "# Let us also save the final model\n",
        "classifier_model.save(os.path.join(OUTPUT_DIR,\"my_classifier_model\"))\n",
        "\n",
        "\n",
        "logger.info(f'You have now finetuned a COVID-Twitter-BERT-model on your dataset. This finetuned model can be found in {OUTPUT_DIR}, and contains these files:')\n",
        "!gsutil ls {OUTPUT_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ586c727o_4",
        "colab_type": "text"
      },
      "source": [
        "# Prediction\n",
        "Let's load the trained model to run some inference. Since the model was saved at the end of the training, it is possible to run this part of the notebook separated. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3X2idiQ7yKQ",
        "colab_type": "text"
      },
      "source": [
        "## Load the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUWEzgde-xqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from official.nlp.bert import tokenization\n",
        "model_name = \"my_classifier_model\"\n",
        "label_mapping = {0: 'negative', 1: 'positive'}  # label names for SST-2\n",
        "\n",
        "model_dir = os.path.join(OUTPUT_DIR,model_name)\n",
        "vocab_file = os.path.join(model_dir, \"assets\",tf.io.gfile.listdir(os.path.join(model_dir,\"assets\"))[0])\n",
        "\n",
        "##Load model and weights\n",
        "classifier_model_saved = tf.keras.models.load_model(model_dir, compile=False)\n",
        "\n",
        "##Create a tokenizer\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
        "\n",
        "max_seq_length = classifier_model_saved.input_shape['input_mask'][1] #Needed for creating examples of correct length\n",
        "num_labels = len(label_mapping)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt5sJKSWlVZ7",
        "colab_type": "text"
      },
      "source": [
        "#Load the model from a checkpoint\n",
        "It is also possible to load the model from the latest checkpoint. We then need to first build the model before loading the weights from the checkpoint. Uncomment the code to run it, and change the last cell to use this model for running the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jP1N29xk5xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "from official.nlp.bert import tokenization\n",
        "from official.nlp.bert import configs as bert_configs\n",
        "assert MODEL_CLASS\n",
        "label_mapping = {0: 'negative', 1: 'positive'}  # label names for SST-2\n",
        "\n",
        "VOCAB_PATH = os.path.join('.', 'vocabs', PRETRAINED_MODELS[MODEL_CLASS]['vocab_file'])\n",
        "CONFIG_FILE = os.path.join('.', 'configs', PRETRAINED_MODELS[MODEL_CLASS]['config'])\n",
        "checkpoint_path = os.path.join(OUTPUT_DIR, 'checkpoint')  # Points to the previously trained model checkpoint\n",
        "\n",
        "##Build model\n",
        "model_config = get_model_config()\n",
        "classifier_model_checkpoint = tf.keras.models.load_model(model_dir, compile=False)\n",
        "\n",
        "##Load weights\n",
        "checkpoint_load_status = classifier_model_checkpoint.load_weights(checkpoint_path)\n",
        "checkpoint_load_status.expect_partial()\n",
        "\n",
        "##Create a tokenizer\n",
        "tokenizer = get_tokenizer(MODEL_CLASS)\n",
        "\n",
        "max_seq_length = 96 #Needed for creating examples of correct length. Set manually or automatically as above\n",
        "num_labels = len(label_mapping)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcglILlDFTg9",
        "colab_type": "text"
      },
      "source": [
        "## Predict helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3sbjh4t9gdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Only needed when restoring checkpoints\n",
        "def get_tokenizer(model_class):\n",
        "    model = PRETRAINED_MODELS[model_class]\n",
        "    tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_PATH, do_lower_case=model['lower_case'])\n",
        "    return tokenizer\n",
        "\n",
        "#Only needed when restoring checkpoints\n",
        "def get_model_config():\n",
        "    \"\"\"Reads BERT config\"\"\"\n",
        "    config = bert_configs.BertConfig.from_json_file(CONFIG_FILE)\n",
        "    return config\n",
        "\n",
        "def create_example(text, tokenizer, max_seq_length):\n",
        "    tokens = ['[CLS]']\n",
        "    input_tokenized = tokenizer.tokenize(text)\n",
        "    if len(input_tokenized) + 2 > max_seq_length:\n",
        "        # truncate\n",
        "        input_tokenized = input_tokenized[:(max_seq_length + 2)]\n",
        "    tokens.extend(input_tokenized)\n",
        "    tokens.append('[SEP]')\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    num_tokens = len(input_ids)\n",
        "    input_mask = num_tokens * [1]\n",
        "    # pad\n",
        "    input_ids += (max_seq_length - num_tokens) * [0]\n",
        "    input_mask += (max_seq_length - num_tokens) * [0]\n",
        "    segment_ids = max_seq_length * [0]\n",
        "    return tf.constant(input_ids, dtype=tf.int32), tf.constant(input_mask, dtype=tf.int32), tf.constant(segment_ids, dtype=tf.int32)\n",
        "\n",
        "def generate_single_example(text, tokenizer, max_seq_length):\n",
        "    example = create_example(text, tokenizer, max_seq_length)\n",
        "    example_features = {\n",
        "        'input_word_ids': example[0][None, :],\n",
        "        'input_mask': example[1][None, :],\n",
        "        'input_type_ids': example[2][None, :]\n",
        "    }\n",
        "    return example_features\n",
        "\n",
        "def format_prediction(preds, label_mapping, label_name):\n",
        "    preds = tf.nn.softmax(preds, axis=1)\n",
        "    formatted_preds = []\n",
        "    for pred in preds.numpy():\n",
        "        # convert to Python types and sort\n",
        "        pred = {label: float(probability) for label, probability in zip(label_mapping.values(), pred)}\n",
        "        pred = {k: v for k, v in sorted(pred.items(), key=lambda item: item[1], reverse=True)}\n",
        "        formatted_preds.append({label_name: list(pred.keys())[0], f'{label_name}_probabilities': pred})\n",
        "    return formatted_preds\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_uy1y_1FlaZ",
        "colab_type": "text"
      },
      "source": [
        "## Run inference\n",
        "Change the text and rerun the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M43DI76_oJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_text = 'The colors of the rainbow'  #@param {type: \"string\"}\n",
        "\n",
        "example = generate_single_example(input_text, tokenizer, max_seq_length)\n",
        "preds = classifier_model_saved.predict(example)\n",
        "#preds = classifier_model_checkpoint.predict(example) #Comment out to use checkpoint restored model\n",
        "\n",
        "formatted_preds = format_prediction(preds, label_mapping, 'sentiment')\n",
        "\n",
        "print('Logits:')\n",
        "print(preds)\n",
        "print('\\nProbabilities:')\n",
        "print(json.dumps(formatted_preds, indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhdjgZWAZ60C",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2020 Per Egil Kummervold and Martin Müller\n"
      ]
    }
  ]
}